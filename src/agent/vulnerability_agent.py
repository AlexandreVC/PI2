"""
AI Agent for Vulnerability Analysis and Risk Management.

This agent provides:
- Vulnerability explanation and context
- Risk assessment and business impact analysis
- Remediation recommendations
- Prioritization based on context
"""

import logging
import json
from typing import List, Dict, Any, Optional
from datetime import datetime

from .llm_client import LLMClient, LLMResponse
from ..models import Vulnerability, VulnerabilitySeverity
from ..dispatcher import Dispatcher, TaskType
from ..enrichment import CVEEnricher, MITREMapper

logger = logging.getLogger(__name__)


class VulnerabilityAgent:
    """
    AI Agent specialized in vulnerability analysis and risk management.

    Features:
    - Contextual vulnerability explanation
    - Risk and business impact assessment
    - Prioritized remediation planning
    - Report generation (executive & technical)
    """

    # System prompt for security expert role
    SYSTEM_PROMPT = """You are a Cyber Security Expert specializing in vulnerability analysis and risk management.

Your role is to:
1. Analyze security vulnerabilities with precision and accuracy
2. Assess business impact and risk levels
3. Provide actionable remediation recommendations
4. Communicate findings clearly to both technical and non-technical audiences

Guidelines:
- Be factual and precise - avoid speculation
- Prioritize findings by criticality
- Consider business context in risk assessments
- Provide specific, actionable remediation steps
- Reference industry standards (CVSS, MITRE ATT&CK, CWE) when applicable

Output format: When asked for JSON, respond with valid JSON only."""

    def __init__(
            self,
            llm_client: Optional[LLMClient] = None,
            dispatcher: Optional[Dispatcher] = None,
            cve_enricher: Optional[CVEEnricher] = None,
            mitre_mapper: Optional[MITREMapper] = None,
            temperature: float = 0.0  # Deterministic for reproducibility
    ):
        """
        Initialize the vulnerability agent.

        Args:
            llm_client: LLM client for AI interactions
            dispatcher: Task dispatcher for model selection
            cve_enricher: CVE enrichment module
            mitre_mapper: MITRE ATT&CK mapper
            temperature: LLM temperature (0 for deterministic)
        """
        self.llm = llm_client or LLMClient()
        self.dispatcher = dispatcher or Dispatcher()
        self.cve_enricher = cve_enricher or CVEEnricher()
        self.mitre_mapper = mitre_mapper or MITREMapper()
        self.temperature = temperature

    def analyze_vulnerability(self, vulnerability: Vulnerability) -> Vulnerability:
        """
        Perform comprehensive analysis of a single vulnerability.

        Args:
            vulnerability: Vulnerability to analyze

        Returns:
            Enriched vulnerability with AI analysis
        """
        # Enrich with CVE data
        vulnerability = self.cve_enricher.enrich_vulnerability(vulnerability)

        # Map to MITRE ATT&CK
        vulnerability = self.mitre_mapper.map_vulnerability(vulnerability)

        # Select appropriate model
        dispatch_result = self.dispatcher.dispatch(
            f"Analyze vulnerability {vulnerability.cve_id or vulnerability.title}",
            vulnerability.description
        )

        # Switch to selected model
        if dispatch_result.model:
            self.llm.switch_model(dispatch_result.model.model_id)

        # Generate AI analysis
        prompt = self._build_analysis_prompt(vulnerability)
        response = self.llm.generate(
            prompt,
            self.SYSTEM_PROMPT,
            self.temperature
        )

        vulnerability.ai_analysis = response.content

        # Generate risk assessment
        risk_prompt = self._build_risk_prompt(vulnerability)
        risk_response = self.llm.generate(
            risk_prompt,
            self.SYSTEM_PROMPT,
            self.temperature
        )

        vulnerability.ai_risk_assessment = risk_response.content

        # Generate business impact
        impact_prompt = self._build_impact_prompt(vulnerability)
        impact_response = self.llm.generate(
            impact_prompt,
            self.SYSTEM_PROMPT,
            self.temperature
        )

        vulnerability.ai_business_impact = impact_response.content

        return vulnerability

    def analyze_vulnerabilities(
            self,
            vulnerabilities: List[Vulnerability],
            batch_size: int = 10
    ) -> List[Vulnerability]:
        """
        Analyze multiple vulnerabilities.

        Args:
            vulnerabilities: List of vulnerabilities
            batch_size: Number to process at once

        Returns:
            List of analyzed vulnerabilities
        """
        analyzed = []

        for vuln in vulnerabilities:
            analyzed.append(self.analyze_vulnerability(vuln))

        return analyzed

    def prioritize_vulnerabilities(
            self,
            vulnerabilities: List[Vulnerability],
            context: Optional[Dict[str, Any]] = None
    ) -> List[Vulnerability]:
        """
        Prioritize vulnerabilities based on risk and context.

        Args:
            vulnerabilities: List of vulnerabilities to prioritize
            context: Optional business context

        Returns:
            Sorted list with priority scores
        """
        # Build prioritization prompt
        vuln_summary = self._build_vuln_summary(vulnerabilities)

        context_str = ""
        if context:
            context_str = f"\nBusiness Context:\n{json.dumps(context, indent=2)}"

        prompt = f"""Analyze and prioritize the following vulnerabilities.

{vuln_summary}
{context_str}

For each vulnerability, assign a priority score from 1 (highest) to 5 (lowest).
Consider:
1. CVSS score and severity
2. Exploit availability
3. Business impact potential
4. Ease of remediation

Respond in JSON format:
{{
    "priorities": [
        {{"id": "vuln_id", "priority": 1, "reason": "explanation"}}
    ]
}}"""

        # Use dispatcher for model selection
        dispatch_result = self.dispatcher.dispatch(
            "Prioritize vulnerabilities based on risk",
            vuln_summary
        )

        if dispatch_result.model:
            self.llm.switch_model(dispatch_result.model.model_id)

        result = self.llm.generate_json(prompt, self.SYSTEM_PROMPT, self.temperature)

        # Apply priorities
        priority_map = {}
        for item in result.get("priorities", []):
            priority_map[item["id"]] = item["priority"]

        for vuln in vulnerabilities:
            vuln.remediation_priority = priority_map.get(vuln.id, 3)

        # Sort by priority
        return sorted(vulnerabilities, key=lambda v: v.remediation_priority)

    def generate_remediation_plan(
            self,
            vulnerabilities: List[Vulnerability]
    ) -> Dict[str, Any]:
        """
        Generate a comprehensive remediation plan.

        Args:
            vulnerabilities: List of vulnerabilities to remediate

        Returns:
            Structured remediation plan
        """
        vuln_summary = self._build_vuln_summary(vulnerabilities)

        prompt = f"""Create a comprehensive remediation plan for the following vulnerabilities:

{vuln_summary}

Structure the plan with:
1. Immediate actions (24-48 hours) - Critical fixes
2. Short-term actions (1-2 weeks) - Important fixes
3. Medium-term actions (1 month) - Moderate fixes
4. Long-term improvements - Strategic recommendations

For each action, specify:
- Target vulnerability/vulnerabilities
- Specific steps to implement
- Required resources
- Verification method

Respond in JSON format:
{{
    "immediate_actions": [
        {{"target": "vuln_id", "action": "description", "steps": [], "verification": ""}}
    ],
    "short_term_actions": [],
    "medium_term_actions": [],
    "long_term_improvements": []
}}"""

        dispatch_result = self.dispatcher.dispatch(
            "Generate remediation plan",
            vuln_summary
        )

        if dispatch_result.model:
            self.llm.switch_model(dispatch_result.model.model_id)

        return self.llm.generate_json(prompt, self.SYSTEM_PROMPT, self.temperature)

    def assess_risk(
            self,
            vulnerabilities: List[Vulnerability],
            asset_value: str = "high"
    ) -> Dict[str, Any]:
        """
        Perform overall risk assessment.

        Args:
            vulnerabilities: List of vulnerabilities
            asset_value: Value of affected assets

        Returns:
            Risk assessment report
        """
        vuln_summary = self._build_vuln_summary(vulnerabilities)

        prompt = f"""Perform a risk assessment for the following vulnerabilities.

Asset Value: {asset_value}

Vulnerabilities:
{vuln_summary}

Provide:
1. Overall risk level (Critical/High/Medium/Low)
2. Key risk factors
3. Potential attack scenarios
4. Impact analysis (Confidentiality, Integrity, Availability)
5. Risk mitigation recommendations

Respond in JSON format:
{{
    "overall_risk": "level",
    "risk_score": 0-100,
    "key_factors": [],
    "attack_scenarios": [],
    "impact": {{
        "confidentiality": "description",
        "integrity": "description",
        "availability": "description"
    }},
    "recommendations": []
}}"""

        dispatch_result = self.dispatcher.dispatch(
            "Risk assessment",
            vuln_summary
        )

        if dispatch_result.model:
            self.llm.switch_model(dispatch_result.model.model_id)

        return self.llm.generate_json(prompt, self.SYSTEM_PROMPT, self.temperature)

    def explain_vulnerability(
            self,
            vulnerability: Vulnerability,
            audience: str = "technical"
    ) -> str:
        """
        Generate vulnerability explanation for specific audience.

        Args:
            vulnerability: Vulnerability to explain
            audience: Target audience ("technical" or "executive")

        Returns:
            Formatted explanation
        """
        audience_context = {
            "technical": "for security administrators and developers. Include technical details, code examples if relevant, and specific configuration changes.",
            "executive": "for management and non-technical stakeholders. Focus on business impact, risk level, and resource requirements. Avoid technical jargon."
        }

        prompt = f"""Explain the following vulnerability {audience_context.get(audience, audience_context['technical'])}

Vulnerability: {vulnerability.title}
CVE: {vulnerability.cve_id or 'N/A'}
CVSS Score: {vulnerability.cvss_score}
Severity: {vulnerability.severity.value}

Description: {vulnerability.description}

Affected:
- Host: {vulnerability.affected_host}
- Service: {vulnerability.affected_service}
- Version: {vulnerability.affected_version}

MITRE ATT&CK:
- Tactics: {', '.join(vulnerability.mitre_tactics) if vulnerability.mitre_tactics else 'N/A'}
- Techniques: {', '.join(vulnerability.mitre_techniques) if vulnerability.mitre_techniques else 'N/A'}

Provide a clear, structured explanation appropriate for the audience."""

        dispatch_result = self.dispatcher.dispatch(
            f"Explain vulnerability for {audience}",
            vulnerability.description
        )

        if dispatch_result.model:
            self.llm.switch_model(dispatch_result.model.model_id)

        response = self.llm.generate(prompt, self.SYSTEM_PROMPT, self.temperature)
        return response.content

    def _build_analysis_prompt(self, vulnerability: Vulnerability) -> str:
        """Build prompt for vulnerability analysis."""
        return f"""Analyze the following security vulnerability:

Title: {vulnerability.title}
CVE: {vulnerability.cve_id or 'N/A'}
CVSS Score: {vulnerability.cvss_score}
Severity: {vulnerability.severity.value}
CWE: {vulnerability.cwe_id or 'N/A'}

Description: {vulnerability.description}

Affected System:
- Host: {vulnerability.affected_host}
- Port: {vulnerability.affected_port}
- Service: {vulnerability.affected_service}
- Product: {vulnerability.affected_product}
- Version: {vulnerability.affected_version}

Exploit Available: {vulnerability.exploit_available}

Provide:
1. Technical analysis of the vulnerability
2. Attack vector explanation
3. Potential impact
4. Recommended mitigations"""

    def _build_risk_prompt(self, vulnerability: Vulnerability) -> str:
        """Build prompt for risk assessment."""
        return f"""Assess the risk level for this vulnerability:

{vulnerability.title} ({vulnerability.cve_id or 'No CVE'})
CVSS: {vulnerability.cvss_score}
Exploit Available: {vulnerability.exploit_available}
Exploitability: {vulnerability.exploitability}

MITRE ATT&CK Context:
- Tactics: {', '.join(vulnerability.mitre_tactics) if vulnerability.mitre_tactics else 'Unknown'}
- Techniques: {', '.join(vulnerability.mitre_techniques) if vulnerability.mitre_techniques else 'Unknown'}

Provide a risk assessment including:
1. Risk level (Critical/High/Medium/Low)
2. Likelihood of exploitation
3. Potential business impact
4. Urgency of remediation"""

    def _build_impact_prompt(self, vulnerability: Vulnerability) -> str:
        """Build prompt for business impact assessment."""
        return f"""Assess the business impact of this vulnerability:

{vulnerability.title}
Severity: {vulnerability.severity.value}
CVSS: {vulnerability.cvss_score}
Affected Service: {vulnerability.affected_service}

Consider impacts on:
1. Business operations
2. Data confidentiality
3. System availability
4. Regulatory compliance
5. Reputation

Provide a concise business impact statement suitable for executive communication."""

    def _build_vuln_summary(self, vulnerabilities: List[Vulnerability]) -> str:
        """Build summary of multiple vulnerabilities."""
        lines = []
        for v in vulnerabilities:
            lines.append(
                f"- [{v.id}] {v.title} | "
                f"CVE: {v.cve_id or 'N/A'} | "
                f"CVSS: {v.cvss_score} | "
                f"Host: {v.affected_host} | "
                f"Exploit: {'Yes' if v.exploit_available else 'No'}"
            )
        return "\n".join(lines)
